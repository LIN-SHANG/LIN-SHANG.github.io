<!doctype html>
<html lang="zh-cn">
  <head>
    <title>毕设设计思路 // Lynn Lawson</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.82.0" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Lynn Lawson" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://LIN-SHANG.github.io/css/main.min.88e7083eff65effb7485b6e6f38d10afbec25093a6fac42d734ce9024d3defbd.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="毕设设计思路"/>
<meta name="twitter:description" content="简介 主要介绍了我的中南大学的毕业设计思路，目前详细阅读了 Multimodal Transformer for Unaligned Multimodal Language Sequences和Learning Modality-Specific Representations with Self-Supervised Multi-Task Learning for Multimodal Sentiment Analysis,毕设想要综合两者的方法，对多模态情感信息(文本、语音和视频)进行情感识别,基本想法是采用第一篇文章中的crossmodal建立起跨模态的长程依赖，让模态信息特征不过于单一，优化特征表示，再用self-mm机制进行自监督学习，进一步减少多模态和单模态预测的差值，进而提高最终的预测结果。
目前跑通了第一篇的代码，正在捋顺第二篇的逻辑，这篇blog里暂时行文比较混乱和粗糙，大致流程为：
介绍了基本数据集，
由于用的数据集是经过CMU研究人员处理过的，介绍了处理的方法，
接着介绍了ULGM自监督方法，第二篇的思路，以及我的困惑。
数据集 CMU-MOSI数据集 CMU-MOSI是一个人类多模态情感分析数据集，它由2199个短的独白视频剪辑片段构成，每个剪辑片段持续一整个序列。CMU-MOSI数据集中的声音和视觉特征分别以12.5和15hz从剪辑片段中采集。（文本数据已经被逐字分割，且已经被glove embedding方法映射成特征向量。
CMU-MOSEI数据集 CMU-MOSEI也是一个情感分析数据集，它由23454个Youtube上面的影评视频剪辑而成。（数据量大概是CMU-MOSI的10倍。未对齐的CMU-MOSEI序列数据集中的声音和视觉特征分别以20Hz和15hz的频率从剪辑视频中采集。
CMU-MOSI &amp; CMU-MOSEI数据集说明 不论是对于CMU-MOSI还是CMU-MOSEI数据，实验人员已经对其中的每个样本都以-3到&#43;3的情感进行了打分。（其中-3代表的是强烈负面情绪，&#43;3代表的是强烈正面情绪）。
同时，我们使用不同的度量标准来对模型进行评估，这些度量标准与之前工作中使用的保持一致：7分类准确率，2分类准确率，F1 分值，绝对平均误差以及模型类人关联率。
IEMOCAP数据集 IEMOCAP是一个由10000个视频组成的人类情感分析数据集。情感标签一共有四种(happy,sad,angry &amp; neutral)。不同于CMU-MOSI和CMU-MOSEI，这是一个多标签任务。IEMOCAP数据集中的声音和视觉特征分别以12.5和15hz从剪辑片段中采集。
特征提取 语言文本 通过预训练的Glove word embedding方法将词语映射成为300维的向量。
视觉特征 使用Facet方法预测了35个面部动作单元，这记录了面部肌肉的移动以代表每帧图像的基本和高级情感。
声音特征 使用COVAREP来提取低维的声音特征。该特征包括12个Mel倒谱系数、基音跟踪和浊音/清音分割特征、声门源参数、峰值斜率参数和最大频散系数。特征维数为74.
ULGM ULGM方法基于多模态情感标注和模态特征表示来产生单一模态的监督值。为了避免来自网络参数更新而产生的不必要的干扰，ULGM方法被设计为一个无参数的模块。通常来讲，单一模态的监督值同多模态标签高度关联。因此，ULGM方法根据模态特征表示到聚类中心的相对距离值来计算偏差值。
相对距离值 由于不同模态的存在于不同的特征空间，用绝对距离值往往不准确。因此，我们提出一种同特征空间差别关联不大的相对距离值来表征偏差。首先，在训练过程中，我们计算不同模态的正情感中心$(C^p_i)$和负情感中心$(C^n_i)$：
$$ C^p_i=\frac{\sum^N_{j=1}I(y_i(j)&gt;0)\cdot F^g_{ij}}{\sum^N_{j=1}I(y_i(j)&gt;0)}\tag{1} $$
$$ C^n_i=\frac{\sum^N_{j=1}I(y_i(j)&lt;0)\cdot F^g_{ij}}{\sum^N_{j=1}I(y_i(j)&lt;0)}\tag{2} $$
式中$i\in{m,t,a,v}$,$N$是训练样本的个数，$I(\cdot)$是指示器函数。$F^g_{ij}$是第$i$个模态中第$j$个样本的全局特征表示。
对于模态特征表示，我们使用$L2$ 归一化来计算$F_i^*$ $$ D^p_i = \frac{||F^*_i-C^p_i||^2_2}{\sqrt{d_i}} \tag{3} $$
$$ D^n_i = \frac{||F^*_i-C^n_i||^2_2}{\sqrt{d_i}} \tag{4} $$"/>

    <meta property="og:title" content="毕设设计思路" />
<meta property="og:description" content="简介 主要介绍了我的中南大学的毕业设计思路，目前详细阅读了 Multimodal Transformer for Unaligned Multimodal Language Sequences和Learning Modality-Specific Representations with Self-Supervised Multi-Task Learning for Multimodal Sentiment Analysis,毕设想要综合两者的方法，对多模态情感信息(文本、语音和视频)进行情感识别,基本想法是采用第一篇文章中的crossmodal建立起跨模态的长程依赖，让模态信息特征不过于单一，优化特征表示，再用self-mm机制进行自监督学习，进一步减少多模态和单模态预测的差值，进而提高最终的预测结果。
目前跑通了第一篇的代码，正在捋顺第二篇的逻辑，这篇blog里暂时行文比较混乱和粗糙，大致流程为：
介绍了基本数据集，
由于用的数据集是经过CMU研究人员处理过的，介绍了处理的方法，
接着介绍了ULGM自监督方法，第二篇的思路，以及我的困惑。
数据集 CMU-MOSI数据集 CMU-MOSI是一个人类多模态情感分析数据集，它由2199个短的独白视频剪辑片段构成，每个剪辑片段持续一整个序列。CMU-MOSI数据集中的声音和视觉特征分别以12.5和15hz从剪辑片段中采集。（文本数据已经被逐字分割，且已经被glove embedding方法映射成特征向量。
CMU-MOSEI数据集 CMU-MOSEI也是一个情感分析数据集，它由23454个Youtube上面的影评视频剪辑而成。（数据量大概是CMU-MOSI的10倍。未对齐的CMU-MOSEI序列数据集中的声音和视觉特征分别以20Hz和15hz的频率从剪辑视频中采集。
CMU-MOSI &amp; CMU-MOSEI数据集说明 不论是对于CMU-MOSI还是CMU-MOSEI数据，实验人员已经对其中的每个样本都以-3到&#43;3的情感进行了打分。（其中-3代表的是强烈负面情绪，&#43;3代表的是强烈正面情绪）。
同时，我们使用不同的度量标准来对模型进行评估，这些度量标准与之前工作中使用的保持一致：7分类准确率，2分类准确率，F1 分值，绝对平均误差以及模型类人关联率。
IEMOCAP数据集 IEMOCAP是一个由10000个视频组成的人类情感分析数据集。情感标签一共有四种(happy,sad,angry &amp; neutral)。不同于CMU-MOSI和CMU-MOSEI，这是一个多标签任务。IEMOCAP数据集中的声音和视觉特征分别以12.5和15hz从剪辑片段中采集。
特征提取 语言文本 通过预训练的Glove word embedding方法将词语映射成为300维的向量。
视觉特征 使用Facet方法预测了35个面部动作单元，这记录了面部肌肉的移动以代表每帧图像的基本和高级情感。
声音特征 使用COVAREP来提取低维的声音特征。该特征包括12个Mel倒谱系数、基音跟踪和浊音/清音分割特征、声门源参数、峰值斜率参数和最大频散系数。特征维数为74.
ULGM ULGM方法基于多模态情感标注和模态特征表示来产生单一模态的监督值。为了避免来自网络参数更新而产生的不必要的干扰，ULGM方法被设计为一个无参数的模块。通常来讲，单一模态的监督值同多模态标签高度关联。因此，ULGM方法根据模态特征表示到聚类中心的相对距离值来计算偏差值。
相对距离值 由于不同模态的存在于不同的特征空间，用绝对距离值往往不准确。因此，我们提出一种同特征空间差别关联不大的相对距离值来表征偏差。首先，在训练过程中，我们计算不同模态的正情感中心$(C^p_i)$和负情感中心$(C^n_i)$：
$$ C^p_i=\frac{\sum^N_{j=1}I(y_i(j)&gt;0)\cdot F^g_{ij}}{\sum^N_{j=1}I(y_i(j)&gt;0)}\tag{1} $$
$$ C^n_i=\frac{\sum^N_{j=1}I(y_i(j)&lt;0)\cdot F^g_{ij}}{\sum^N_{j=1}I(y_i(j)&lt;0)}\tag{2} $$
式中$i\in{m,t,a,v}$,$N$是训练样本的个数，$I(\cdot)$是指示器函数。$F^g_{ij}$是第$i$个模态中第$j$个样本的全局特征表示。
对于模态特征表示，我们使用$L2$ 归一化来计算$F_i^*$ $$ D^p_i = \frac{||F^*_i-C^p_i||^2_2}{\sqrt{d_i}} \tag{3} $$
$$ D^n_i = \frac{||F^*_i-C^n_i||^2_2}{\sqrt{d_i}} \tag{4} $$" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://LIN-SHANG.github.io/post/%E6%AF%95%E8%AE%BE%E8%AE%BE%E8%AE%A1%E6%80%9D%E8%B7%AF/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-04-05T12:36:37&#43;08:00" />
<meta property="article:modified_time" content="2021-04-05T12:36:37&#43;08:00" />



  </head>
  <body>
    <header class="app-header">
      <a href="https://LIN-SHANG.github.io"><img class="app-header-avatar" src="/text.jpg" alt="Lynn Lawson" /></a>
      <h1>Lynn Lawson</h1>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">Home</a>
             - 
          
          <a class="app-header-menu-item" href="/tags/">Tags</a>
             - 
          
          <a class="app-header-menu-item" href="/about/">About</a>
      </nav>
      <p>Less is more.</p>
      <div class="app-header-social">
        
          <a href="https://github.com/LIN-SHANG" target="_blank" rel="noreferrer noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>Github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg>
          </a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">毕设设计思路</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Apr 5, 2021
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          2 min read
        </div>
      </div>
    </header>
    <div class="post-content">
      <h1 id="简介">简介</h1>
<p>主要介绍了我的中南大学的毕业设计思路，目前详细阅读了
<a href="https://arxiv.org/abs/1906.00295">Multimodal Transformer for Unaligned Multimodal Language Sequences</a>和<a href="https://arxiv.org/abs/2102.04830v1">Learning Modality-Specific Representations with Self-Supervised Multi-Task Learning for Multimodal Sentiment Analysis</a>,毕设想要综合两者的方法，对多模态情感信息(文本、语音和视频)进行情感识别,基本想法是采用第一篇文章中的crossmodal建立起跨模态的长程依赖，让模态信息特征不过于单一，优化特征表示，再用self-mm机制进行自监督学习，进一步减少多模态和单模态预测的差值，进而提高最终的预测结果。</p>
<p>目前跑通了第一篇的代码，正在捋顺第二篇的逻辑，这篇blog里暂时行文比较混乱和粗糙，大致流程为：</p>
<p>介绍了基本数据集，</p>
<p>由于用的数据集是经过CMU研究人员处理过的，介绍了处理的方法，</p>
<p>接着介绍了ULGM自监督方法，第二篇的思路，以及我的困惑。</p>
<h1 id="数据集">数据集</h1>
<h3 id="cmu-mosi数据集">CMU-MOSI数据集</h3>
<p>CMU-MOSI是一个人类多模态情感分析数据集，它由2199个短的独白视频剪辑片段构成，每个剪辑片段持续一整个序列。CMU-MOSI数据集中的声音和视觉特征分别以12.5和15hz从剪辑片段中采集。（文本数据已经被逐字分割，且已经被glove embedding方法映射成特征向量。</p>
<h3 id="cmu-mosei数据集">CMU-MOSEI数据集</h3>
<p>CMU-MOSEI也是一个情感分析数据集，它由23454个Youtube上面的影评视频剪辑而成。（数据量大概是CMU-MOSI的10倍。未对齐的CMU-MOSEI序列数据集中的声音和视觉特征分别以20Hz和15hz的频率从剪辑视频中采集。</p>
<h3 id="cmu-mosi--cmu-mosei数据集说明">CMU-MOSI &amp; CMU-MOSEI数据集说明</h3>
<p>不论是对于CMU-MOSI还是CMU-MOSEI数据，实验人员已经对其中的每个样本都以-3到+3的情感进行了打分。（其中-3代表的是强烈负面情绪，+3代表的是强烈正面情绪）。</p>
<p>同时，我们使用不同的度量标准来对模型进行评估，这些度量标准与之前工作中使用的保持一致：7分类准确率，2分类准确率，F1 分值，绝对平均误差以及模型类人关联率。</p>
<h3 id="iemocap数据集">IEMOCAP数据集</h3>
<p>IEMOCAP是一个由10000个视频组成的人类情感分析数据集。情感标签一共有四种(happy,sad,angry &amp;  neutral)。不同于CMU-MOSI和CMU-MOSEI，这是一个多标签任务。IEMOCAP数据集中的声音和视觉特征分别以12.5和15hz从剪辑片段中采集。</p>
<h1 id="特征提取">特征提取</h1>
<h3 id="语言文本">语言文本</h3>
<p>通过预训练的Glove word embedding方法将词语映射成为300维的向量。</p>
<h3 id="视觉特征">视觉特征</h3>
<p>使用Facet方法预测了35个面部动作单元，这记录了面部肌肉的移动以代表每帧图像的基本和高级情感。</p>
<h3 id="声音特征">声音特征</h3>
<p>使用COVAREP来提取低维的声音特征。该特征包括12个Mel倒谱系数、基音跟踪和浊音/清音分割特征、声门源参数、峰值斜率参数和最大频散系数。特征维数为74.</p>
<h1 id="ulgm">ULGM</h1>
<p>ULGM方法基于多模态情感标注和模态特征表示来产生单一模态的监督值。为了避免来自网络参数更新而产生的不必要的干扰，ULGM方法被设计为一个无参数的模块。通常来讲，单一模态的监督值同多模态标签高度关联。因此，ULGM方法根据模态特征表示到聚类中心的相对距离值来计算偏差值。</p>
<h3 id="相对距离值">相对距离值</h3>
<p>由于不同模态的存在于不同的特征空间，用绝对距离值往往不准确。因此，我们提出一种同特征空间差别关联不大的相对距离值来表征偏差。首先，在训练过程中，我们计算不同模态的正情感中心$(C^p_i)$和负情感中心$(C^n_i)$：</p>
<p>$$
C^p_i=\frac{\sum^N_{j=1}I(y_i(j)&gt;0)\cdot F^g_{ij}}{\sum^N_{j=1}I(y_i(j)&gt;0)}\tag{1}
$$</p>
<p>$$
C^n_i=\frac{\sum^N_{j=1}I(y_i(j)&lt;0)\cdot F^g_{ij}}{\sum^N_{j=1}I(y_i(j)&lt;0)}\tag{2}
$$</p>
<p>式中$i\in{m,t,a,v}$,$N$是训练样本的个数，$I(\cdot)$是指示器函数。$F^g_{ij}$是第$i$个模态中第$j$个样本的全局特征表示。</p>
<p>对于模态特征表示，我们使用$L2$ 归一化来计算$F_i^*$
$$
D^p_i = \frac{||F^*_i-C^p_i||^2_2}{\sqrt{d_i}} \tag{3}
$$</p>
<p>$$
D^n_i = \frac{||F^*_i-C^n_i||^2_2}{\sqrt{d_i}} \tag{4}
$$</p>
<p>公式中$i\in{m,t,a,v}$。$d_i$是特征维度，一个缩放因子。</p>
<p>然后，我们定义了一个相对距离值，其衡量了从模态特征值到正情感中心和负情感中心的相对距离。
$$
\alpha_i=\frac{D^n_i-D^p_i}{D^p_i+\epsilon}=\frac{D^n_i+\epsilon}{D^p_i+\epsilon}-1 \tag{5}
$$
公式中$i\in{m,t,a,v}$。$\epsilon$是为防止分母为零而引入的一个很小的数字。</p>
<h3 id="偏移误差">偏移误差</h3>
<p>很直观的可以看见$\alpha_i$正比于最后的结果。故我们通过以下式子让监督值和预测值建立起联系：
$$
\frac{y_s}{y_m}\propto\frac{\hat{y}_s}{\hat{y}_m}\propto\frac{\alpha_s}{\alpha_m}\Rightarrow
y_s = \frac{\alpha_s*y_m}{\alpha_m} \tag{6}
$$</p>
<p>$$
y_s-y_m\propto\hat{y}_s-\hat{y}_m\propto\alpha_s-\alpha_m\Rightarrow y_s = y_m+\alpha_s-\alpha_m \tag{7}
$$</p>
<p>其中$s\in{t,a,v}$。</p>
<p>具体而言，公式7的引入是为了避免“0值问题”。在公式6中，当$y_m$等于0时，产生的单模态监督值$y_s$通常也是零。然后联合考虑以上关系，我们通过等权求和能够得到单模态监督值：
$$
\begin{align}
y_s &amp;= \frac{y_m*\alpha_s}{2\alpha_m}+\frac{y_m+\alpha_s-\alpha_m}{2}   \\<br>
&amp;= y_m+\frac{\alpha_s-\alpha_m}{2}*\frac{y_m+\alpha_m}{\alpha_m}   \\<br>
&amp;= y_m + \delta_{sm}
\end{align}
\tag{8}
$$
其中$s\in{t,a,v}$。$\delta_{sm} = \frac{\alpha_s-\alpha_m}{2}*\frac{y_m+\alpha_m}{\alpha_m}$代表着单模态监督值到多模态标签值的误差值。</p>
<h3 id="基于动量的更新策略">基于动量的更新策略</h3>
<p>鉴于模态表示的动态更新，通过等式8计算产生的单模态标签往往不够稳固。为了减轻这中不利影响，我们设计了一个基于动量的更新策略，其同时兼顾了新产生的值和历史值。
$$
y_s^{(i)}=
\begin{cases}
y_m,&amp;\text{$i$=1}  \\<br>
\frac{i-1}{i+1}y_s^{i-1}+\frac{2}{i+1}y^i_s,&amp;\text{$i$&gt;1} \\<br>
\end{cases}
\tag{9}
$$
其中$s\in \{t,a,v\}$。$y_s^i$是在$i_{th}$轮迭代新产生的单模态标签。$y_s^{(i)}$是$i_{th}$轮迭代后最终单模态标签。</p>
<p>形式上，假设总迭代次数是$n$,推算可得$y_s^i$的权重是$\frac{2i}{(n)(n+1)}$。这意味着在更新过程中，后产生的单模态标签参与模态标签更新的权重大于之前产生的权重。这同我们的日常经验相符合。因为产生的单模态标签是之前所有更新的累计和，故此标签在经历足够多的迭代次数后会趋于稳定(在我们的实验中次数大概是20次)。然后，单模态任务的训练过程会逐渐变得稳固。单模态标签更新的策略在算法一中予以展示。</p>
<h3 id="优化函数">优化函数</h3>
<p>最后我们使用$L1Loss$作为基本的优化函数。对于单模态任务，我们使用单标签和多标签的差别作为损失函数的权重。这表明网络应该更加关注差异较大的样本。</p>
<p>$$
L = \frac{1}{N}\sum^N_i(|\hat{y}^i_m-y^i_m|+\sum^{{t,a,v}}_{s}W^i_s*|\hat{y}^i_s-y^{(i)}_s|)
\tag{10}
$$</p>
<p>公式中$N$是训练样本的个数，$W^i_s = tanh(|y_s^{(i)}-y_m|)$是辅助任务$s$的第$i_{th}$个样本的权重。</p>
<hr>
<h3 id="algorithm-1-unimodal-supervisions-update-policy"><strong>Algorithm 1</strong> Unimodal Supervisions Update Policy</h3>
<hr>
<blockquote>
<p><strong>Input:</strong>  unimodal inputs $I_t$,$I_a$,$I_v$,m-labels $y_m$</p>
<p><strong>output:</strong>  u-labels$y_t^{(i)}$,$y_a^{(i)}$,$y_v^{(i)}$,where $i$ means the number of training epochs</p>
<p>1：Initialize model parameters M($\theta$;$x$)</p>
<p>2:  Initialize u-labels $y_t^{(1)}=y_m$,$y_a^{(1)}=y_m$,$y_v^{(1)}=y_m$</p>
<p>3:  Initialize global representations $F_t^g=0$,$F_a^g=0$,$F_v^g=0$,$F_m^g=0$</p>
<p>4:  <strong>for</strong> $n\in[1,end]$ <strong>do</strong></p>
<p>5:  	<strong>for</strong>  mini-batch in dataLoader <strong>do</strong></p>
<p>6:  		Compute mini-batch modality representations$F_t^*$,$F_a^*$,$F_v^*$,$F_m^*$</p>
<p>7:  		Compute loss $L$ using Equation(10)</p>
<p>8:		  Compute parameters gradient $\frac{\partial L}{\partial \theta}$</p>
<p>9:  		Update model parameters: $\theta = \theta - \eta\frac{\partial L}{\partial \theta}$</p>
<p>10:  	  <strong>if</strong>  $n\neq1$  then</p>
<p>11: 			Compute relative distance values $\alpha_m$,$\alpha_t$,$\alpha_a$,and $\alpha_v$ using Equation(1-5)</p>
<p>12:			Compute $y_t$,$y_a$,$y_v$ using Equation(8)</p>
<p>13:			 Update $y_t^{(n)}$,$y_a^{(n)}$,$y_v^{(n)}$ using Equation(9)</p>
<p>14:		 <strong>end if</strong></p>
<p>15:		 Update glabal  representation $F_s^g$ using $F_s^*$, where $s \in {m,t,a,v}$</p>
<p>16:	 <strong>end for</strong></p>
<p>17:  <strong>end for</strong></p>
</blockquote>
<p><img src="https://z3.ax1x.com/2021/04/05/cQ6sVs.md.png" alt="cQ6sVs.md.png"></p>
<h1 id="对模型的理解">对模型的理解</h1>
<p>我认为模型的更新分为两个步骤：</p>
<p>1.模型的参数更新</p>
<p>2.特征向量的更新</p>
<h3 id="模型的参数更新">模型的参数更新</h3>
<p>当raw_data 经过初步的特征提取为一些初始test,audio,video特征向量之后，分别会经过Bert，和两个sLSTM，这三个特征提取模块进行更抽象的特征提取。其中：
$$
F_t = BERT(I_t;\theta_t^{bert})\in R^{d_t}
$$
$$
F_a = sLSTM(I_a;\theta_a^{lstm})\in R^{d_t}
$$
$$
F_v = sLSTM(I_v;\theta_a^{lstm})\in R^{d_v}
$$
然后通过简单的concatenate形成$F_m$,同时为了更加准确的预测，采用一个线性映射将$F_m$通过下式：
$$
F_m^* = ReLU(W_{l1}^{mT}[F_t;F_a;F_v]+b_{l1}^m)
$$
得到融合特征表示$F_m^*$,，其被用来做最后的情感预测：
$$
\hat{y}_m = W_{l2}^{mT}F_m^*+b_{l1}^m
$$
很显然单单只是如此简单的一个特征提取加上特征拼接以及线性映射就作为最终的预测向量，精确度会大打折扣。于是作者引入了自监督学习机制。</p>
<h3 id="想法来源">想法来源</h3>
<p>通过对$F_t$,$F_a$,$F_v$进行线性映射可以得到$F_t^*$,$F_a^*$,$F_v^*$,通过对比研究发现，多模态的融合特征表示$F_m^*$与正情感中心的距离相较于负情感中心更近，而单模态的特征表示$F_s^*$则反之，然而，经验上来说，如果能够让$y_s$同$y_m$之间的偏差$\delta_{sm}$趋于0，即让单模态和多模态的预测结果尽量一致，让各自偏差估计相抵消，对最终的预测结果$\hat{y}_m$大有裨益。ULGM应运而生，通过该方法可以通过自监督的机制对特征向量进行迭代更新。</p>
<p><img src="https://z3.ax1x.com/2021/04/05/cQ6Dbj.png" alt="cQ6Dbj.png"></p>
<h3 id="特征向量的更新">特征向量的更新</h3>
<p>在计算了$F_t$,$F_a$,$F_v$之后，同多模态任务一样，经过两次线性映射分别得到$F_t^*$,$F_a^*$,$F_v^*$和 $\hat{y}_t$,$\hat{y}_a$,$\hat{y}_v$。然后根据公式10计算loss函数。同时让loss函数对$\theta$参数进行求偏导，之后对于$\theta$进行参数更新。</p>
<p>如果这是第一次迭代计算， 用 $F_s^*$更新$F_s^g$, 其中 $s \in {m,t,a,v}$。</p>
<p>如果这不是第一次迭代计算，用公式(1-5)计算 $\alpha_m$,$\alpha_t$,$\alpha_a$,和$\alpha_v$，再用公式（8）计算$y_t$,$y_a$,$y_v$ ，最后用公式(9)（基于动量的更新策略）计算$y_t^{(n)}$,$y_a^{(n)}$,$y_v^{(n)}$ ，同时再用$F_s^*$更新$F_s^g$, 其中 $s \in {m,t,a,v}$。</p>
<h3 id="我的困惑">我的困惑</h3>
<ol>
<li>除了计算Loss，$\hat{y}_t$,$\hat{y}_a$,$\hat{y}_v$意义何在？</li>
<li>loss函数反向求导更新参数的时候，多个linear的计算矩阵参数也被更新了吗？更新的参数是不是只有Bert和两个LSTM网络那一块儿？</li>
<li>在计算$\alpha_m$,$\alpha_t$,$\alpha_a$,和$\alpha_v$的时候，会用到一整个batch的数据，但这个时候只能够更新一个batch中的一个样本的特征向量，是不是说每次更新后都会重新计算$\alpha_m$,$\alpha_t$,$\alpha_a$,和$\alpha_v$？</li>
<li>我想将MulT和self-mm方法结合一下，即用crossmodal提取特征，然后用self-mm的机制进行自监督学习，是否可行？</li>
</ol>

    </div>
    <div class="post-footer">
      
      
<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #eee8e8;
}
</style>
    </div>
  </article>

    </main>
  </body>
</html>
